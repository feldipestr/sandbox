# spark-data-lake-etl

Use this **ETL** framework to run Apache Spark batch pipelines in Apache Airflow. **Extract** data from an Amazon S3 Data Lake, **Transform** data by Spark SQL and **Load** data to a columnar key-value storage. 

